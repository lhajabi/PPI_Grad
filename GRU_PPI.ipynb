{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lhajabi/PPI_Grad/blob/main/GRU_PPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7WcTlfeQBFK",
        "outputId": "b1fa465a-250a-49c4-b638-8b065f7dc038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/128.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m112.6/128.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0M3JgllR5Ses"
      },
      "outputs": [],
      "source": [
        "!pip3 install keras-visualizer\n",
        "from keras_visualizer import visualizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4-4R6uwaVfB"
      },
      "outputs": [],
      "source": [
        "\"\"\"ONLY RUNS IF GPU AVAILABLE\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "\n",
        "gpu()\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imports"
      ],
      "metadata": {
        "id": "A_gX6z2sjn5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {tf.keras.__version__ if hasattr(tf.keras, '__version__') else tf.keras.__path__[0].split('/')[-1]}\")\n",
        "print(f\"Standalone Keras version: {keras.__version__}\")\n"
      ],
      "metadata": {
        "id": "NlFkkRHgjn5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9G7RuHfjn5h"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers import LSTM, GRU, Input, concatenate, Flatten, BatchNormalization, Dense, Dropout, Embedding, Reshape, RNN, SimpleRNN,Bidirectional\n",
        "from keras.models import Model,Sequential\n",
        "from keras.callbacks import Callback,EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score,roc_curve,auc,matthews_corrcoef,accuracy_score\n",
        "from keras.metrics import Precision, Recall,Accuracy\n",
        "from keras import layers"
      ],
      "metadata": {
        "id": "bKsvLf0hjn5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FI2C1Xhywxt"
      },
      "source": [
        "#Data Preperation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Duw7Nfm0yp8b",
        "outputId": "7aea30b0-28f2-4c4e-e262-3d219ba8be93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGBgtPnTzBpx"
      },
      "outputs": [],
      "source": [
        "df_links = pd.read_csv('/content/drive/MyDrive/Data/GENERATED DATASETS/Generated100k_(POS40-0.7)_50:50.txt', delimiter=',')\n",
        "df_links.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBFTWEma1mW6"
      },
      "source": [
        "tokenization and padding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRwx-vC1PxHo"
      },
      "outputs": [],
      "source": [
        "def generate_word_token_dict(amino_acids_list, k):\n",
        "    words = set(product(amino_acids_list, repeat=k))\n",
        "    words = [''.join(word) for word in words]\n",
        "    tokens = [int(i) for i in range(1, len(words) + 1)]\n",
        "    #num_digits = len(str(max(tokens)))\n",
        "    #tokens = [f\"{token:0{num_digits}}\" for token in tokens]\n",
        "    word_token = dict(zip(words, tokens))\n",
        "    return word_token\n",
        "\n",
        "def tokenization(cell, word_token, n_grams):\n",
        "  if len(cell)/n_grams != 0:\n",
        "    cell_0 = cell[len(cell)%n_grams:]\n",
        "  cell_1 = [cell_0[i:i+n_grams] for i in range(0, len(cell_0), n_grams)]\n",
        "  #print(cell_1)\n",
        "  cell_1 = [word_token[word] for word in cell_1]\n",
        "  #cell = ''.join(cell_1)\n",
        "  cell = cell_1\n",
        "  return cell\n",
        "\n",
        "def pad(cell, fixed_length):\n",
        "    if len(cell) > fixed_length:\n",
        "        # If the original list is longer, remove leftmost elements\n",
        "        return cell[-fixed_length:]\n",
        "    elif len(cell) < fixed_length:\n",
        "        # If the original list is shorter, pad leftmost with zeroes\n",
        "        return [0] * (fixed_length - len(cell)) + cell\n",
        "    else:\n",
        "        # If the original list has the desired length, no modifications are needed\n",
        "        return cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKnQNGBbXrAW"
      },
      "outputs": [],
      "source": [
        "amino_acids_list = ['A','B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y','Z']\n",
        "n_grams = 3\n",
        "fixed_length = 1000\n",
        "word_token = generate_word_token_dict(amino_acids_list, k=n_grams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0id7lVH0RDQl"
      },
      "outputs": [],
      "source": [
        "\n",
        "columns = ['protein1', 'protein2']\n",
        "for column in columns:\n",
        "  df_links[column] = df_links[column].apply(func = tokenization, args = (word_token, n_grams,))\n",
        "  df_links[column] = df_links[column].apply(func= pad, args = (fixed_length,))\n",
        "df_links.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5tmASTGWCi0"
      },
      "outputs": [],
      "source": [
        "df_links.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrTKDTcVoQ9b"
      },
      "source": [
        "#Split Data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ntGOMGRlLX5"
      },
      "outputs": [],
      "source": [
        "x_data = df_links[['protein1', 'protein2']]\n",
        "y_data = df_links['combined_score']\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(x_data, y_data, test_size=0.3, random_state=42, shuffle=True, stratify=None)\n",
        "x_validate, x_test, y_validate, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=43, shuffle=True, stratify=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c381RihKgiG3"
      },
      "outputs": [],
      "source": [
        "print(x_train.info())\n",
        "print(x_validate.info())\n",
        "print(x_test.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUIhSgFwlLX6"
      },
      "outputs": [],
      "source": [
        "input_1_train = np.array(x_train['protein1'].values.tolist())\n",
        "input_2_train = np.array(x_train['protein2'].values.tolist())\n",
        "output_train = np.array(y_train)\n",
        "\n",
        "input_1_validate = np.array(x_validate['protein1'].values.tolist())\n",
        "input_2_validate = np.array(x_validate['protein2'].values.tolist())\n",
        "output_validate = np.array(y_validate)\n",
        "\n",
        "input_1_test = np.array(x_test['protein1'].values.tolist())\n",
        "input_2_test = np.array(x_test['protein2'].values.tolist())\n",
        "output_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwoOBRfJxl0b"
      },
      "outputs": [],
      "source": [
        "x_train = [input_1_train, input_2_train]\n",
        "y_train = output_train\n",
        "\n",
        "x_val = [input_1_validate, input_2_validate]\n",
        "y_val = output_validate\n",
        "\n",
        "x_test = [input_1_test, input_2_test]\n",
        "y_test = output_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWqe2LMqW4rS"
      },
      "outputs": [],
      "source": [
        "\"\"\"y_train = y_train.astype('float32')\n",
        "y_val = y_val.astype('float32')\n",
        "y_test = y_test.astype('float32')\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtpr0c0nn2EE"
      },
      "source": [
        "# Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6B95Hb6YVgPZ"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wqSAZExy6xV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao7fJW1Pyiza"
      },
      "outputs": [],
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6EoSSPRAL-8"
      },
      "source": [
        "#Metrics and Callbacks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0z8WlzVhKW-3"
      },
      "outputs": [],
      "source": [
        "thresholds = [0.3, 0.5, 0.7, 0.9]\n",
        "metrics = [ #Available metrics for binary_crossentropy are: loss,precision,recall,auc,auc_1,val_loss,val_precision,val_recall,val_auc,val_auc_1\n",
        "    keras.metrics.BinaryAccuracy(threshold=0.5),\n",
        "    keras.metrics.Accuracy(),\n",
        "    keras.metrics.Precision(thresholds = thresholds),\n",
        "    keras.metrics.Recall(thresholds = thresholds),\n",
        "    keras.metrics.AUC(curve = 'ROC', name = 'ROC'),\n",
        "    keras.metrics.AUC(curve=\"PR\", name = 'PR'),\n",
        "    #f1_score,\n",
        "    keras.metrics.BinaryCrossentropy()\n",
        "    ]\n",
        "loss_function = 'binary_crossentropy'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "\"\"\"def f1_score(y_true, y_pred):\n",
        "    precision = Precision()(y_true, y_pred)\n",
        "    recall = Recall()(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
        "\"\"\"\n",
        "def calculate_metrics(y_true, y_pred, threshold):\n",
        "    y_pred_binary = (y_pred > threshold).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_binary).ravel()\n",
        "\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred_binary)\n",
        "    accuracy = accuracy_score(y_true, y_pred_binary)\n",
        "    precision = precision_score(y_true, y_pred_binary)\n",
        "    recall = recall_score(y_true, y_pred_binary)\n",
        "    f1 = f1_score(y_true, y_pred_binary)\n",
        "    auc = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "    return sensitivity, specificity, mcc, accuracy, precision, recall, f1, auc"
      ],
      "metadata": {
        "id": "wPBRZiNlKJz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_threshold(model, x_val, y_val):\n",
        "    y_pred = model.predict(x_val)\n",
        "\n",
        "    # Generate a range of thresholds\n",
        "    thresholds = np.arange(0, 1.01, 0.01)\n",
        "    best_threshold = None\n",
        "    best_mcc = -1\n",
        "\n",
        "    # Iterate through thresholds and find the best one\n",
        "    for threshold in thresholds:\n",
        "        sens,spec,mcc,acc,prec, rec, f1,auc = calculate_metrics(y_val, y_pred, threshold)\n",
        "        if mcc > best_mcc:\n",
        "            best_mcc = mcc\n",
        "            best_threshold = threshold\n",
        "\n",
        "    return best_threshold,best_mcc"
      ],
      "metadata": {
        "id": "v9slFRjZ7ssc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJYlZ5UzAL-8"
      },
      "outputs": [],
      "source": [
        "class ConfusionMatrixCallback(keras.callbacks.Callback):\n",
        "  def __init__(self, x_val, y_val):\n",
        "        super(ConfusionMatrixCallback, self).__init__()\n",
        "        self.x_val, self.y_val = x_val, y_val\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "        predictions = self.model.predict(x = x_val)\n",
        "        # Convert probabilities to classes using the threshold\n",
        "        predictions = (predictions >= 0.5).astype(int)\n",
        "        cm = confusion_matrix(y_val, predictions)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)\n",
        "        plt.title('Confusion Matrix - Epoch {}'.format(epoch))\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rate_model(y_true, y_pred, cm_threshold = 0.5):\n",
        "  for m in metrics:\n",
        "    m.update_state(y_true, y_pred)\n",
        "    print('{} : {}'.format(m.name, m.result()))\n",
        "  y_pred = (y_pred >= cm_threshold).astype(int)\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('True')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "YjKYVfl_jbqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "9UPBUFiQsuSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor= 'val_loss' , factor=0.2, patience=0.1, min_lr=0.001, verbose=1)\n",
        "confusion_matrix_callback = ConfusionMatrixCallback(x_val, y_val)\n",
        "early_stopping = EarlyStopping(monitor= 'val_loss', patience=5, restore_best_weights = True)\n",
        "callbacks = [early_stopping, reduce_lr, confusion_matrix_callback]"
      ],
      "metadata": {
        "id": "DwWN6e9jjuie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9sQQDT_33Tg"
      },
      "source": [
        "#GRU_1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LpzZiKH8ED0"
      },
      "outputs": [],
      "source": [
        "#inputs\n",
        "protein_1 = tf.keras.layers.Input(shape=(fixed_length,), name='protein1')\n",
        "protein_2 = tf.keras.layers.Input(shape=(fixed_length,), name='protein2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxR_bnCf37nd"
      },
      "outputs": [],
      "source": [
        "#shared embedding layer\n",
        "embedding = tf.keras.layers.Embedding(input_dim = len(word_token) + 1, output_dim = 512, mask_zero = 1,  input_length=fixed_length, name = 'shared_embedding_0')\n",
        "embedding_1 = embedding(protein_1)\n",
        "embedding_2 = embedding(protein_2)\n",
        "#concatenated_embeddings = concatenate([embedding_1, embedding_2], name = 'shared_embedding_1')\n",
        "model_0 = Model(inputs = [protein_1, protein_2], outputs = [embedding_1,embedding_2])\n",
        "\n",
        "model_1 = tf.keras.Sequential(name = 'sequential')\n",
        "model_1.add(tf.keras.layers.BatchNormalization(name = 'bn_0', input_shape = (fixed_length,512)))\n",
        "model_1.add(tf.keras.layers.GRU(name = 'gru', units = 64, activation='tanh', recurrent_activation='sigmoid', go_backwards = True , return_sequences= False ))\n",
        "model_1.add(tf.keras.layers.Dense( 64, activation='elu',name = 'dense'))\n",
        "model_1.add(tf.keras.layers.BatchNormalization(name = 'bn_1'))\n",
        "model_1.add(tf.keras.layers.Dropout(0, name = 'dropout'))\n",
        "\n",
        "feature_array_128 = tf.keras.layers.concatenate([model_1(model_0.outputs[0]), model_1(model_0.outputs[1])])\n",
        "x = tf.keras.layers.Dense(512, activation='elu')(feature_array_128)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "x = tf.keras.layers.Dense(1, activation='sigmoid')(x) #i added this because we changed the loss function\n",
        "final_model = Model(inputs = [protein_1, protein_2], outputs = x)\n",
        "final_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYdU2K-pVTj7"
      },
      "source": [
        "#GRU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kTWm2nRRTx5"
      },
      "outputs": [],
      "source": [
        "#inputs\n",
        "protein_1 = tf.keras.layers.Input(shape=(fixed_length,), name='protein1')\n",
        "protein_2 = tf.keras.layers.Input(shape=(fixed_length,), name='protein2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGokpPaDhGKN"
      },
      "outputs": [],
      "source": [
        "embedding = tf.keras.layers.Embedding(input_dim = len(word_token) + 1, output_dim = 512, mask_zero = 1,  input_length=fixed_length,name='Embedding')\n",
        "embedding_1 = embedding(protein_1)\n",
        "embedding_2 = embedding(protein_2)\n",
        "model_0 = Model(inputs = [protein_1, protein_2], outputs = [embedding_1,embedding_2])\n",
        "\n",
        "model_1 = Sequential()\n",
        "model_1.add(BatchNormalization(name = 'BatchNormalization0' ,input_shape = (fixed_length,512)))\n",
        "model_1.add(GRU(name = 'GRU_Layer',units = 64, activation='tanh', recurrent_activation='sigmoid'))\n",
        "model_1.add(Dense( 64, activation='elu',name = 'Dense'))\n",
        "model_1.add(BatchNormalization( name = 'BatchNormalizatio1n' ))\n",
        "model_1.add(Dropout(0.4))\n",
        "\n",
        "feature_array_128 = concatenate([model_1(model_0.outputs[0]), model_1(model_0.outputs[1])])\n",
        "x = Dense(512, activation='elu',name = 'Dense0')(feature_array_128)\n",
        "x = BatchNormalization(name = 'BatchNormalization' )(x)\n",
        "x = Dense(1, activation='sigmoid',name = 'Dense1')(x)\n",
        "final_model = Model(inputs = [protein_1, protein_2], outputs = x)\n",
        "final_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CG9RtChO88_6"
      },
      "outputs": [],
      "source": [
        " #visualizer(final_model, file_format='png', view=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6tEZbNKU51K1",
        "outputId": "fdb300da-b5a6-4fad-8dfc-1d3ac52123fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"from tensorflow.keras.utils import plot_model\\nplot_model(final_model, to_file='model.png', show_shapes=True, show_layer_names=True)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "\"\"\"from tensorflow.keras.utils import plot_model\n",
        "plot_model(final_model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJgndkoKi2KD"
      },
      "outputs": [],
      "source": [
        "final_model.compile(optimizer='adam',loss=loss_function, metrics = metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ0oYt7OSR-t"
      },
      "outputs": [],
      "source": [
        "final_model.load_weights('/content/drive/MyDrive/saved_models/2024_01_06_04_18/final_weights_GRU3.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaUIbOMZO5Md"
      },
      "source": [
        "#Training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjMa27bWSHFU"
      },
      "outputs": [],
      "source": [
        "final_model.fit(x_train, y_train, validation_data = (x_val, y_val), epochs = 3, callbacks = callbacks,batch_size=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bypqs_AcTat5"
      },
      "outputs": [],
      "source": [
        "timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
        "drive_path = os.path.join('/content/drive/MyDrive/saved_models', timestamp)\n",
        "!mkdir -p $drive_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKulrl2SnOx3"
      },
      "outputs": [],
      "source": [
        "final_model.save_weights(os.path.join(drive_path, f'final_weights_GRU3.h5'))\n",
        "final_model.save(os.path.join(drive_path, f'final_model_GRU3.h5'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt5CIBhlpaGl"
      },
      "source": [
        "#TensorBoard:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4UKgTLb9fKI"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDMNxbDWHfKf"
      },
      "source": [
        "#GRU Bidirectional:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue_rOvo_QGjq"
      },
      "outputs": [],
      "source": [
        "#inputs\n",
        "protein_1 = tf.keras.layers.Input(shape=(fixed_length,), name='protein1')\n",
        "protein_2 = tf.keras.layers.Input(shape=(fixed_length,), name='protein2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2K-NzdXHrry"
      },
      "outputs": [],
      "source": [
        "embedding = tf.keras.layers.Embedding(input_dim = len(word_token) + 1, output_dim = 512, mask_zero = 1,  input_length=fixed_length)\n",
        "embedding_1 = embedding(protein_1)\n",
        "embedding_2 = embedding(protein_2)\n",
        "model_0 = Model(inputs = [protein_1, protein_2], outputs = [embedding_1,embedding_2])\n",
        "\n",
        "model_1 = Sequential()\n",
        "model_1.add(BatchNormalization(input_shape = (fixed_length,512)))\n",
        "model_1.add(Bidirectional(GRU(name = 'GRU_Layer',units = 64, activation='tanh', recurrent_activation='sigmoid')))\n",
        "model_1.add(Dense( 64, activation='elu'))\n",
        "model_1.add(BatchNormalization())\n",
        "model_1.add(Dropout(0.6))\n",
        "\n",
        "feature_array_128 = concatenate([model_1(model_0.outputs[0]), model_1(model_0.outputs[1])])\n",
        "x = Dense(512, activation='elu')(feature_array_128)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(1, activation='sigmoid')(x)\n",
        "final_model = Model(inputs = [protein_1, protein_2], outputs = x)\n",
        "final_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjQ-yzduOULt"
      },
      "outputs": [],
      "source": [
        "final_model.compile(optimizer='adam',loss=loss_function, metrics = metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pntQ1zw3OULu"
      },
      "outputs": [],
      "source": [
        "final_model.load_weights('/content/drive/MyDrive/saved_models/2024_01_09_03_05/final_weights_GRUB3.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYNce77YOchY"
      },
      "outputs": [],
      "source": [
        "final_model.fit(x_train, y_train, validation_data = (x_val, y_val), epochs = 3, callbacks = callbacks,batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw9wyLI4N3Pb"
      },
      "outputs": [],
      "source": [
        "timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
        "drive_path = os.path.join('/content/drive/MyDrive/saved_models', timestamp)\n",
        "!mkdir -p $drive_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdPBeBG3N3Pc",
        "outputId": "2c9037d6-ccf5-491b-cae7-0d7a9a29757f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "final_model.save_weights(os.path.join(drive_path, f'final_weights_GRUB3.h5'))\n",
        "final_model.save(os.path.join(drive_path, f'final_model_GRUB3.h5'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rate model performance:"
      ],
      "metadata": {
        "id": "svCLQDXfjbqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Find the best threshold based on best mcc score on val data\n",
        "best_threshold,best_MCC = find_best_threshold(final_model, x_val, y_val)\n",
        "print(f'Best Matthews Correlation Coefficient: {best_MCC} at Threshold: {best_threshold}')\n"
      ],
      "metadata": {
        "id": "gozuyll94FPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_with_threshold(model, x_test, y_test, threshold):\n",
        "    y_pred = model.predict(x_test)\n",
        "    sens,spec,mcc,acc,prec, rec, f1,auc = calculate_metrics(y_test, y_pred, threshold)\n",
        "    print(f'For Threshold:{threshold}:\\nMCC = {mcc}\\nsensitivity = {sens}\\nspecificity = {spec}\\naccuracy = {acc}\\nprecision = {prec}\\nrecall = {rec}\\nf1-score = {f1}\\nauc = {auc}')\n",
        "\n",
        "# Evaluate the model with the best threshold\n",
        "evaluate_with_threshold(final_model, x_test, y_test, best_threshold)\n"
      ],
      "metadata": {
        "id": "Ah95oxhyCP_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1iaJoIHNS1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d0962d-4c0a-4302-dcc3-6da4f117a319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "469/469 [==============================] - 210s 448ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = final_model.predict(x = x_test, callbacks = None )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rate_model(y_test, predictions)"
      ],
      "metadata": {
        "id": "JD1bQ9onjbqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOS1fSOmPqjw"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Load the evaluation results\n",
        "evaluation_results_path = os.path.join(drive_path, 'evaluation_results.json')\n",
        "with open(evaluation_results_path, 'r') as file:\n",
        "    evaluation_results = json.load(file)\n",
        "\"\"\"\n",
        "y_true = output_test\n",
        "y_pred = predictions\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "\n",
        "# Compute area under the curve (AUC)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9HN8wWchO5E"
      },
      "source": [
        "#Load model & weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LC_H5PFPZ97"
      },
      "outputs": [],
      "source": [
        "#run to load pretrained weights\n",
        "new_model = final_model\n",
        "new_model.load_weights('/content/drive/MyDrive/saved_models/2024_01_02_20_28/final_weights_GRU5.h5')\n",
        "#new_model.summary()\n",
        "new_model.compile(optimizer='adam',loss=loss_function, metrics = metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hJoPMKgheLq",
        "outputId": "f6191712-f3a1-4efd-f159-8974187cd89a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ]
        }
      ],
      "source": [
        "new_model.fit(x_train, y_train, validation_data = (x_val, y_val), epochs = 3, callbacks = callbacks,batch_size=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fe9AV7hI7Mf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
        "drive_path = os.path.join('/content/drive/MyDrive/saved_models', timestamp)\n",
        "!mkdir -p $drive_path\n",
        "\n",
        "new_model.save_weights(os.path.join(drive_path, f'final_weights_GRU6.h5'))\n",
        "new_model.save(drive_path,'final_model_GRU6.h5')\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed3A6nmOX9HU"
      },
      "outputs": [],
      "source": [
        "evaluate_train_0 = new_model.evaluate(x = x_test_temp, y = y_test_temp, callbacks = None )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4ZE3ga3wj34"
      },
      "outputs": [],
      "source": [
        "predictions_0 = new_model.predict(x = x_test, callbacks = None )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bx1z3mVfhD5b"
      },
      "outputs": [],
      "source": [
        "def rate_model(y_true, y_pred, cm_threshold = 0.5):\n",
        "  for m in metrics:\n",
        "    m.update_state(y_true, y_pred)\n",
        "    print('{} : {}'.format(m.name, m.result()))\n",
        "  y_pred = (y_pred >= cm_threshold).astype(int)\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('True')\n",
        "  plt.show()\n",
        "\n",
        "rate_model(y_test, predictions_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zko-yxAmZ5T1"
      },
      "source": [
        "#Tester:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgA0TNlNZ9tE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7393f382-9ad5-48cd-c10c-a2ff6f00f350"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model_GRU_final.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(final_model, 'model_GRU_final.joblib', protocol=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbZdwCS2aHHB"
      },
      "outputs": [],
      "source": [
        "import cloudpickle\n",
        "\n",
        "with open('model_GRU_final.pkl', 'wb') as file:\n",
        "    cloudpickle.dump(final_model, file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jBtj8f3GzaeO",
        "-FI2C1Xhywxt",
        "XrTKDTcVoQ9b",
        "GgtQ22w8hxkN",
        "dtpr0c0nn2EE",
        "x9sQQDT_33Tg",
        "m9HN8wWchO5E",
        "MDMNxbDWHfKf",
        "5y9-7pX6fZCn"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}